# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UyerM382KPtaBelMbJ7l2Ch6ejkPSliC
"""

!pip install datasets

import pandas as pd
from pathlib import Path
import numpy as np
from datasets import Dataset, DatasetDict
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import TrainingArguments, Trainer

import pandas as pd
from google.colab import drive

drive.mount('/content/drive')

train_df=pd.read_csv('/content/drive/MyDrive/us-patent-phrase-to-phrase-matching/train.csv')
test_df=pd.read_csv('/content/drive/MyDrive/us-patent-phrase-to-phrase-matching/test.csv')

train_df.head()

test_df.head()

print(train_df['score'].value_counts())

import matplotlib.pyplot as plt
plt.hist(train_df['score'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Similarity Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

train_df.describe(include = 'object')

train_df.anchor.value_counts()

train_df.target.value_counts()

train_df['input'] = 'text1: '+ train_df.context + ';text2: ' + train_df.anchor + ';text3: '+train_df.target

train_df.head()

train_df.input.head()

ds = Dataset.from_pandas(train_df)

ds

!ls

!pwd

!pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = 'microsoft/deberta-v3-small'

toke = AutoTokenizer.from_pretrained(model_name)

"""The model microsoft/deberta-v3-small refers to the DeBERTa (Decoding-enhanced BERT with disentangled attention) model, which is a smaller version of the DeBERTa model developed by Microsoft Research.

DeBERTa is an extension of BERT (Bidirectional Encoder Representations from Transformers), a popular model for natural language understanding tasks. DeBERTa improves upon BERT by introducing several enhancements, including disentangled attention mechanisms, more efficient training, and better handling of long sequences.

Here's a brief overview of the key features of DeBERTa:

Disentangled Attention: DeBERTa employs disentangled self-attention mechanisms that allow the model to attend to different aspects of the input separately. This can improve the model's ability to capture complex relationships between tokens in the input sequence.

More Efficient Training: DeBERTa introduces several training techniques to improve efficiency and convergence speed. These techniques include gradient checkpointing, layer normalization, and layer-wise learning rate scaling.

Handling Long Sequences: DeBERTa is designed to handle long input sequences more effectively than traditional Transformer models like BERT. It achieves this by using hierarchical representations and incorporating positional embeddings that capture the relative positions of tokens within long sequences.

Overall, DeBERTa aims to enhance the performance of BERT-based models on various natural language understanding tasks by addressing some of the limitations and challenges associated with standard Transformer architectures.
"""

model = AutoModelForSequenceClassification.from_pretrained(model_name)

tokens = toke.tokenize("Akash is learning NLP")
print(tokens)

def tokenizer(x):
    return toke(x['input'])

tok_ds = ds.map(tokenizer, batched=True)

tok_ds[0]



"""# in many machine learning frameworks, the target variable or the label is commonly referred to as "labels". Renaming the column to "labels" makes it consistent with this convention, which can be helpful when using the dataset for training a machine learning model. It ensures clarity and consistency in the dataset structure, especially if you plan to use it with libraries that expect the target variable to be named "labels"."""

tok_ds = tok_ds.rename_columns({"score": "labels"})

dds = tok_ds.train_test_split(0.00001)
dds

test_df.head(2)

test_df['input'] = 'text1: ' + test_df.context + '; text2: ' +test_df.target + '; text3: ' + test_df.anchor

test_ds = Dataset.from_pandas(test_df).map(tokenizer, batched=True)

test_ds

bs = 128
epochs = 4
lr = 8e-5

"""Batch size (bs): It refers to the number of training examples utilized in one iteration. In this case, bs = 128 means that the training data will be divided into batches, each containing 128 examples. These batches will be fed into the model sequentially during training.

Epochs: An epoch is one complete pass through the entire training dataset. If epochs = 4, it means that during training, the model will iterate over the entire dataset four times. Each epoch consists of multiple iterations, where each iteration processes one batch of data.

Learning rate (lr): It determines the step size at which the model parameters are updated during training. A higher learning rate allows the model to learn faster but may risk overshooting the optimal solution. Conversely, a lower learning rate may lead to slower convergence but can yield more accurate results. In this case, lr = 8e-5 means that the learning rate is set to 8e-5, which is a relatively small value often used in fine-tuning pre-trained models like BERT.
"""

def corr(x,y): return np.corrcoef(x,y)[0][1]

"""This function, corr(x, y), calculates the Pearson correlation coefficient between two arrays x and y. The Pearson correlation coefficient is a measure of the linear correlation between two variables. It ranges from -1 to 1, where:

1 indicates a perfect positive linear relationship,
-1 indicates a perfect negative linear relationship, and
0 indicates no linear relationship between the variables.
The function np.corrcoef(x, y) calculates the correlation matrix between x and y, and [0][1] indexes the correlation coefficient between x and y from the correlation matrix.

So, corr(x, y) returns the Pearson correlation coefficient between arrays x and y.
"""

def corr_d(eval_pred):
    return {'pearson': corr(*eval_pred)}

pip install wandb

import wandb

wandb.init(mode='disabled')

model_path='/content/drive/MyDrive/us-patent-phrase-to-phrase-matching/model.safetensors'

def get_trainer(dds):
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)

    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,
    evaluation_strategy="epoch", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,
    num_train_epochs=epochs, weight_decay=0.01, report_to='none')


    return Trainer(model,args, train_dataset=dds['train'], eval_dataset=dds['test'], tokenizer=toke, compute_metrics=corr_d)

trainer = get_trainer(dds);
trainer.train()

"""/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/huggingfacedebertav3variants/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 [1140/1140 05:32, Epoch 4/4]
Epoch	Training Loss	Validation Loss	Pearson
1	No log	0.000788	nan
2	0.038300	0.004281	nan
3	0.038300	0.002197	nan
4	0.014700	0.001359	nan
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice
  c = cov(x, y, rowvar, dtype=dtype)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice
  c = cov(x, y, rowvar, dtype=dtype)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice
  c = cov(x, y, rowvar, dtype=dtype)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply
  c *= np.true_divide(1, fact)
TrainOutput(global_step=1140, training_loss=0.024701679484886035, metrics={'train_runtime': 333.4708, 'train_samples_per_second': 437.484, 'train_steps_per_second': 3.419, 'total_flos': 963183448052640.0, 'train_loss': 0.024701679484886035, 'epoch': 4.0})
"""

pred = trainer.predict(eval_ds).predictions.astype(float)
pred

array([[ 0.57177734],
       [ 0.72753906],
       [ 0.31982422],
       [ 0.42333984],
       [ 0.00608063],
       [ 0.48632812],
       [ 0.49560547],
       [ 0.16918945],
       [ 0.41430664],
       [ 1.08886719],
       [ 0.23486328],
       [ 0.21594238],
       [ 0.64990234],
       [ 0.79052734],
       [ 0.76269531],
       [ 0.18212891],
       [ 0.28857422],
       [ 0.29174805],
       [ 0.34863281],
       [ 0.27246094],
       [ 0.4921875 ],
       [ 0.33251953],
       [ 0.45043945],
       [ 0.07476807],
       [ 0.55224609],
       [-0.02304077],
       [ 0.18762207],
       [ 0.37915039],
       [ 0.04440308],
       [ 0.74609375],
       [ 0.16223145],
       [ 0.11291504],
       [ 0.62792969],
       [ 0.46948242],
       [ 0.46337891],
       [ 0.24353027]])

pred = np.clip(pred,0,1)

pred[1][0]

def one_dem(pred):
    preds = []
    for o in pred:
        preds.append(o[0])
    return preds

preds = one_dem(pred)
len(preds)

import datasets
submission = datasets.Dataset.from_dict({
    'id': eval_ds['id'],
    'score': preds,
})
submission.to_csv('submission.csv',index=False)

sub = pd.read_csv('submission.csv')

id	score
0	4112d61851461f60	0.571777
1	09e418c93a776564	0.727539
2	36baf228038e314b	0.319824
3	1f37ead645e7f0c8	0.423340
4	71a5b6ad068d531f	0.006081
5	474c874d0c07bd21	0.486328
6	442c114ed5c4e3c9	0.495605
7	b8ae62ea5e1d8bdb	0.169189
8	faaddaf8fcba8a3f	0.414307
9	ae0262c02566d2ce	1.000000
10	a8808e31641e856d	0.234863
11	16ae4b99d3601e60	0.215942
12	25c555ca3d5a2092	0.649902
13	5203a36c501f1b7c	0.790527
14	b9fdc772bb8fd61c	0.762695
15	7aa5908a77a7ec24	0.182129
16	d19ef3979396d47e	0.288574
17	fd83613b7843f5e1	0.291748
18	2a619016908bfa45	0.348633
19	733979d75f59770d	0.272461
20	6546846df17f9800	0.492188
21	3ff0e7a35015be69	0.332520
22	12ca31f018a2e2b9	0.450439
23	03ba802ed4029e4d	0.074768
24	c404f8b378cbb008	0.552246
25	78243984c02a72e4	0.000000
26	de51114bc0faec3e	0.187622
27	7e3aff857f056bf9	0.379150
28	26c3c6dc6174b589	0.044403
29	b892011ab2e2cabc	0.746094
30	8247ff562ca185cc	0.162231
31	c057aecbba832387	0.112915
32	9f2279ce667b21dc	0.627930
33	b9ea2b06a878df6f	0.469482
34	79795133c30ef097	0.463379
35	25522ee5411e63e9	0.243530

